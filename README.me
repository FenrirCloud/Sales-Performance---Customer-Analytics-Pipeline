# üìä End-to-End E-commerce Analytics Pipeline on GCP

A scalable, cloud-native data pipeline that automates the ingestion, transformation, and analysis of e-commerce data, culminating in an interactive business intelligence dashboard.

![GCP](https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)
![Airflow](https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=Apache-Airflow&logoColor=white)
![BigQuery](https://img.shields.io/badge/BigQuery-669DF6?style=for-the-badge&logo=google-bigquery&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Looker](https://img.shields.io/badge/Looker_Studio-4285F4?style=for-the-badge&logo=looker&logoColor=white)

---

## ‚ú® Final Dashboard

*A preview of the interactive dashboard built with Looker Studio, providing key insights into sales performance and customer segmentation.*

> **(Suggestion: Take a screenshot of your completed Looker Studio dashboard and place it here!)**
>
> `![Looker Studio Dashboard](looker_studio/dashboard_screenshots/final_dashboard.png)`

---

## üöÄ Key Features

*   **‚òÅÔ∏è Cloud-Native Architecture:** Built entirely on Google Cloud Platform for scalability and reliability.
*   **‚öôÔ∏è Automated ETL Orchestration:** Uses Apache Airflow to manage the full workflow with scheduling, dependencies, and retries.
*   **üìà Advanced Customer Segmentation:** Implements **RFM (Recency, Frequency, Monetary) analysis** with complex SQL to categorize customers into segments like 'Champions', 'At Risk', and 'Loyal'.
*   **‚≠ê Dimensional Data Modeling:** Transforms raw data into a query-optimized Star Schema, perfect for analytical workloads.
*   **üìä Interactive Visualization:** Connects the clean, aggregated data to Looker Studio for insightful business reporting.

---

## üèóÔ∏è Architecture Diagram

The pipeline follows a modern ETL approach, moving data from a raw landing zone to a structured data warehouse and finally to aggregated analytics tables for fast dashboarding.

```mermaid
graph TD;
    A[üìÇ Raw CSV Data in GCS Bucket] -->|Orchestrated by| B(üå¨Ô∏è Cloud Composer / Airflow);
    B -->|1. Ingest & Stage| C{üóÇÔ∏è BigQuery Staging Dataset};
    C -->|`ecom_staging_dag`| D[üßº Cleaned Staging Table];
    D -->|2. DWH Transformation| E{‚≠ê BigQuery DWH Dataset};
    subgraph "Star Schema"
        E_dim1[dim_customers]
        E_dim2[dim_products]
        E_dim3[dim_dates]
        E_fact[fact_sales]
    end
    B -->|`ecom_dwh_dag`| E;
    E -->|3. Analytics Aggregation| F{üìà BigQuery Analytics Dataset};
    subgraph "Aggregated Tables"
       F_agg1[agg_daily_sales]
       F_agg2[agg_customer_segments]
       F_agg3[top_n_products_monthly]
    end
    B -->|`ecom_analytics_dag`| F;
    F -->|4. Visualization| G[üìä Looker Studio Dashboard];




üõ†Ô∏è Tech Stack
Category	Technology
Cloud Platform	<img src="https://img.shields.io/badge/Google_Cloud-4285F4?style=flat&logo=google-cloud&logoColor=white" />
Orchestration	<img src="https://img.shields.io/badge/Apache_Airflow-017CEE?style=flat&logo=Apache-Airflow&logoColor=white" />
Data Lake	<img src="https://img.shields.io/badge/Cloud_Storage-4285F4?style=flat&logo=google-cloud&logoColor=white" />
Data Warehouse	<img src="https://img.shields.io/badge/BigQuery-669DF6?style=flat&logo=google-bigquery&logoColor=white" />
Visualization	<img src="https://img.shields.io/badge/Looker_Studio-4285F4?style=flat&logo=looker&logoColor=white" />
Language	<img src="https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white" /> <img src="https://img.shields.io/badge/SQL-4479A1?style=flat&logo=postgresql&logoColor=white" />



üíæ Data Model (Star Schema)

The data is modeled into a Star Schema within the ecommerce_dwh BigQuery dataset to optimize for analytical queries.

    Dimension: dim_customers

    Stores unique customer data, enhanced with RFM segmentation.
    Field	Description
    customer_id	Primary Key - Unique identifier for a customer.
    rfm_segment	Derived segment (e.g., 'Champions', 'At Risk').
    Dimension: dim_products

    Stores unique product information.
    Field	Description
    stock_code	Primary Key - Unique identifier for a product.
    product_category	Derived category based on product description.
    Fact Table: fact_sales

    The central table containing all transaction events.
    Field	Description
    invoice_no	Primary Key - Unique identifier for a sale.
    date_id	Foreign Key to dim_dates.
    customer_id	Foreign Key to dim_customers.
    stock_code	Foreign Key to dim_products.
    line_item_total_amount	Calculated metric (quantity * unit_price).

    üöÄ Getting Started

Follow these steps to replicate the project environment.
Prerequisites

    A Google Cloud Platform project with billing enabled.

    A running Cloud Composer 2 environment.

    Enabled APIs: Cloud Composer, BigQuery, Cloud Storage.

    gcloud CLI and gsutil tools installed (or use Cloud Shell).

1. Create GCP Resources

    Create GCS Bucket for Raw Data:
    code Bash

    
gsutil mb -p YOUR_PROJECT_ID gs://your-raw-data-bucket-name

  

Create BigQuery Datasets:
code Bash

        
    bq mk --location=US ecommerce_staging
    bq mk --location=US ecommerce_dwh
    bq mk --location=US ecommerce_analytics

      

2. Configure & Deploy

    Configure: Update config/gcp_config.json with your project and bucket details.

    Upload Data: Download the Online Retail Dataset and upload it:
    code Bash

    
gsutil cp data/data.csv gs://your-raw-data-bucket-name/raw_sales/sample_e_commerce_data.csv

  

Upload Code to Composer: Sync the project folders to your environment's DAGs bucket:
code Bash

        
    # Find your DAGs bucket URI in the Composer environment details page
    gsutil -m rsync -r ./dags gs://<YOUR_COMPOSER_DAGS_BUCKET_URI>
    gsutil -m rsync -r ./sql gs://<YOUR_COMPOSER_DAGS_BUCKET_URI>/sql
    gsutil -m rsync -r ./config gs://<YOUR_COMPOSER_DAGS_BUCKET_URI>/config

      

3. Run and Visualize

    Open the Airflow UI from the Cloud Composer page.

    Un-pause the four ecom_* DAGs.

    Trigger the pipeline by running the ecom_ingestion_dag.

    Once the pipeline completes, connect Looker Studio to the ecommerce_analytics BigQuery dataset and build your dashboards!

üéØ Key Skills Demonstrated

Cloud Architecture (GCP): Provisioning and managing core data services.

Data Orchestration (Airflow): Building complex, dependent ETL workflows.

Data Warehousing (BigQuery): Schema design, partitioning, and query optimization.

Data Modeling: Applying dimensional modeling (Star Schema) for BI.

Advanced SQL: Writing complex queries with CTEs, window functions, and aggregations.

ETL/ELT Development: Implementing robust ingestion, cleaning, and transformation logic.

Problem Solving: Debugging cloud infrastructure, data quality, and pipeline dependencies.
